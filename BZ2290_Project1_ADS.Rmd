---
title: "R Notebook For Applied Data Science-Project I"
output:
  html_document: default
  html_notebook: default
---
```{r}

```

```{r}
#NECESSARY PACKAGES
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
library(RCurl)
library(ggplot2)
library(rvest)
library(tibble)
library(qdap)
library(sentimentr)
library(gplots)
library(syuzhet)
library(factoextra)
library(beeswarm)
library(scales)
library(RANN)
library(topicmodels)
library(wordcloud)
library(tidytext)
library(d3heatmap)
library(wordcloud2)
library(shinythemes)
```

```{r}
##PREPRCESSIONING OF THE DATA##
folder.path="D:/Columbia University/Spring2017-Applied Data Science/Project_1_Bz2290/Spr2017-Proj1-bz2290/data/InauguralSpeeches"
#Replace to your own path 
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path))
#SPEECH SELECTED
speech_selected=c("AbrahamLincoln-1","AbrahamLincoln-2","WoodrowWilson-1","WoodrowWilson-2","FranklinDroosevelt-2","FranklinDroosevelt-3","FranklinDroosevelt-4","JohnFKenndy-1","RonaldReagan-1","GeorgeWBush-2")
#SPLIT SPEECH INTO SENTENCES


#REFERENCE:
#LINCOLN-1 1 LINCOLN-2 2 
#WoodrowWilson-1 56 WoodrowWilson-2 57
#FranklinDroosevelt-2 13 FranklinDroosevelt-3 14 FranklinDroosevelt-4 15
#JohnFKenndy-1 35
#RonaldReagan-1 41
#GeorgeWBush-2 21
L1=sent_detect(ff.all[[1]]$content,endmarks = c("?", ".", "!", "|",";"))
L2=sent_detect(ff.all[[2]]$content,endmarks = c("?", ".", "!", "|",";"))
W1=sent_detect(ff.all[[56]]$content,endmarks = c("?", ".", "!", "|",";"))
W2=sent_detect(ff.all[[57]]$content,endmarks = c("?", ".", "!", "|",";"))
F2=sent_detect(ff.all[[13]]$content,endmarks = c("?", ".", "!", "|",";"))
F3=sent_detect(ff.all[[14]]$content,endmarks = c("?", ".", "!", "|",";"))
F4=sent_detect(ff.all[[15]]$content,endmarks = c("?", ".", "!", "|",";"))
J1=sent_detect(ff.all[[35]]$content,endmarks = c("?", ".", "!", "|",";"))
R1=sent_detect(ff.all[[41]]$content,endmarks = c("?", ".", "!", "|",";"))
G2=sent_detect(ff.all[[21]]$content,endmarks= c("?", ".", "!", "|",";"))
#Create Emotion Matrix
sentence.list = list(L1,L2,W1,W2,F2,F3,F4,J1,R1,G2)
title.list = c(rep("AbrahamLincoln-1",length(L1)),rep("AbrahamLincoln-2",length(L2)),rep("WoodrowWilson-1",length(W1)),rep("WoodrowWilson-2",length(W2)),rep("FranklinDroosevelt-2",length(F2)),rep("FranklinDroosevelt-3",length(F3)),rep("FranklinDroosevelt-4",length(F4)),rep("JohnFKenndy-1",length(J1)),rep("RonaldReagan-1",length(R1)),rep("GeorgeWBush-2",length(G2)))
emotion.matrix = cbind(title.list,c(L1,L2,W1,W2,F2,F3,F4,J1,R1,G2))
colnames(emotion.matrix)=c("President","Sentences")
interm.matrix = NULL
for(i in 1 : length(sentence.list))
{
  emotions=diag(1/(word_count(sentence.list[[i]])+0.01))%*%as.matrix(get_nrc_sentiment(sentence.list[[i]]))
  colnames(emotions)=c( "anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust", "negative", "positive")
 interm.matrix = rbind(interm.matrix,cbind(sent.id=as.numeric(1:length(sentence.list[[i]])),nword=word_count(sentence.list[[i]]),as.matrix(emotions)))
}
emotion.matrix = as.data.frame(cbind(emotion.matrix,interm.matrix))

Factor_to_Numeric=function(df)
{
  for(i in 3:14)
  {
    df[,i] = as.numeric(as.character(df[,i]))
  }
  return(df)
}
emotion.matrix = Factor_to_Numeric(emotion.matrix)

```

```{r}
#Return Short Sentence
return_short_sentence=function(df=emotion.matrix,Presidents,nwords=8)
{
  return(df$Sentences[which(df$President==Presidents & df$nword <= nwords)])
}
return_short_sentence(President = "AbrahamLincoln-1")
return_short_sentence(President = "AbrahamLincoln-2")
#WoodrowWilson-1
return_short_sentence(President = "WoodrowWilson-1")
return_short_sentence(President = "WoodrowWilson-2")

return_short_sentence(President = "FranklinDroosevelt-2")
return_short_sentence(President = "FranklinDroosevelt-3")
return_short_sentence(President = "FranklinDroosevelt-4")

return_short_sentence(President = "JohnFKenndy-1")

return_short_sentence(President = "RonaldReagan-1")

return_short_sentence(Presidents = "GeorgeWBush-2")
```

```{r}
#Length of Sentences
sel.comparison=c("AbrahamLincoln-1",
                 "AbrahamLincoln-2",
                 "WoodrowWilson-1",
                 "WoodrowWilson-2",
                 "FranklinDroosevelt-2",
                 "FranklinDroosevelt-3",
                 "FranklinDroosevelt-4",
                 "JohnFKenndy-1",
                 "RonaldReagan-1",
                 "GeorgeWBush-2")
sentence.length = emotion.matrix[,1:4]
sentence.length$PoP = factor(sentence.length$President)
sentence.length$Ordered=reorder(sentence.length$PoP, 
                                  sentence.length$nword, 
                                  mean, 
                                  order=T)
sentence.length= as.data.frame(sentence.length)
beeswarm(nword~Ordered, 
         data=sentence.length,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.length$Ordered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Nomination speeches")
```

```{r}
library(shiny)
shinyApp(
  ui = fluidPage(
    sidebarLayout(
      sidebarPanel(
      checkboxGroupInput("POP","President",speech_selected,selected = speech_selected[1]),
      shinythemes::themeSelector()),
      mainPanel(
      plotOutput("beeswarm")))
  ),
  server = function(input,output,session){
    selected.data = reactive(
      {
        sentence = emotion.matrix[,1:4]
        sentence = subset(sentence,as.character(sentence$President) %in% input$POP)
        sentence$Pop = factor(sentence$President)
        sentence$Ordered=reorder(sentence$Pop,sentence$nword,mean,order=T)
        return(sentence)

      }
    )
    output$beeswarm = renderPlot(
      {
        beeswarm(nword~Ordered, 
         data=selected.data(),
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.length$Ordered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Nomination speeches")
      }
    )
  }
)
```

```{r,eval=FALSE,include=FALSE}
#Clustering of emotions#
heatmap.2(cor(emotion.matrix%>%filter(President==speech_selected[6])%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
```

```{r,eval=FALSE,include=FALSE}
heatmap.2(cor(emotion.matrix%>%filter(President==speech_selected[8])%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100), , margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
```
```{r}
d3heatmap(cor(emotion.matrix%>%filter(President==speech_selected[9])%>%select(anger:trust)),Rowv = FALSE, Colv=FALSE)
```
```{r}
#write.csv(as.matrix(dtm),file="dtm.csv")
A = emotion.matrix[emotion.matrix$President==speech_selected[4],5:14]
B = emotion.matrix[emotion.matrix$President==speech_selected[6],5:14]
C = emotion.matrix[emotion.matrix$President==speech_selected[8],5:14]
D = emotion.matrix[emotion.matrix$President==speech_selected[9],5:14]
write.csv(as.matrix(A),file="A4.csv")
write.csv(as.matrix(B),file="B6.csv")
write.csv(as.matrix(C),file="C8.csv")
write.csv(as.matrix(D),file="D9.csv")
emotion.matrix$anger[emotion.matrix$President=="RonaldReagan-1"]

```

```{r}
#Emtion Charged Sentences
Emotion_Sentences = function(df=emotion.matrix,pop)
{
  df=tbl_df(emotion.matrix)%>%
  filter(President==pop,nword>=4)%>%
  select(Sentences, anger:trust)
  df=as.data.frame(df)
  index=apply(df[,-1], 2, which.max)
  result = NULL
  for(i in 1:length(index))
  {
    result = rbind(result,as.character(df$Sentences[index[i]]))
  }
  result = cbind(c("anger","anticipation","disgust","fear","joy","sadness","surprise","trust"),result)
  return(result)
}

print(speech_selected[1])
Emotion_Sentences(pop=speech_selected[1])

print(speech_selected[2])
Emotion_Sentences(pop=speech_selected[2])

print(speech_selected[3])
Emotion_Sentences(pop=speech_selected[3])

print(speech_selected[4])
Emotion_Sentences(pop=speech_selected[4])

print(speech_selected[5])
Emotion_Sentences(pop=speech_selected[5])

print(speech_selected[6])
Emotion_Sentences(pop=speech_selected[6])

print(speech_selected[7])
Emotion_Sentences(pop=speech_selected[7])

print(speech_selected[8])
Emotion_Sentences(pop=speech_selected[8])

print(speech_selected[9])
Emotion_Sentences(pop=speech_selected[9])

print(speech_selected[10])
Emotion_Sentences(pop=speech_selected[10])
```
```{r}
#Emotion Barplot
Emotion_Barplot= function(pop)
{
  par(mar=c(4, 6, 2, 1))
  emo.means=colMeans(select(subset(emotion.matrix,emotion.matrix$President==pop), anger:trust)>0.01,na.rm=TRUE)
  col.use=c("red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1")
  barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
}
Emotion_Barplot(speech_selected[8])

```
```{r}
corpus.list=emotion.matrix[2:(nrow(emotion.matrix)-1), ]
sentence.pre=emotion.matrix$Sentences[1:(nrow(emotion.matrix)-2)]
sentence.post=emotion.matrix$Sentences[3:(nrow(emotion.matrix)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## Text mining
```{r}
docs <- Corpus(VectorSource(corpus.list$snipets))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

### Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r}
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

### Topic modeling

Gengerate document-term matrices. 

```{r}
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

```

Run LDA

```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 15

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("D:/Columbia University/Spring2017-Applied Data Science/Project_1_Bz2290/Spr2017-Proj1-bz2290/LDAGibbs",k,"DocsToTopics.csv"))

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("D:/Columbia University/Spring2017-Applied Data Science/Project_1_Bz2290/Spr2017-Proj1-bz2290/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("D:/Columbia University/Spring2017-Applied Data Science/Project_1_Bz2290/Spr2017-Proj1-bz2290/LDAGibbs",k,"TopicProbabilities.csv"))
```

```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```

Based on the most popular terms and the most salient terms for each topic, we assign a hashtag to each topic. This part require manual setup as the topics are likely to change. 

```{r}
topics.hash=c("Government","Spirit","Nation","Democracy","Determination","Freedom","Misc","America","Arm","Justice","Citizen","Fear","Equality","Constitution","Strive")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
write.csv(as.matrix(corpus.list.df),file="topic.csv")
```

## Clustering of topics
```{r, fig.width=3, fig.height=4}
#par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
              select(President, Government:Strive)%>%
              group_by(President)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
hot.topic=apply(topic.summary[,2:16],1,which.max)

colnames(topic.summary)[hot.topic+1]
# [1] "Economy"         "America"         "Defense"         "Belief"         
# [5] "Election"        "Patriotism"      "Unity"           "Government"     
# [9] "Reform"          "Temporal"        "WorkingFamilies" "Freedom"        
# [13] "Equality"        "Misc"            "Legislation"       


topic.plot=c(1, 13, 9, 11, 8, 3, 7)
print(topics.hash[topic.plot])

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
          trace = "none", density.info = "none")
d3heatmap()
```

```{r, fig.width=3.3, fig.height=5}
# [1] "Economy"         "America"         "Defense"         "Belief"         
# [5] "Election"        "Patriotism"      "Unity"           "Government"     
# [9] "Reform"          "Temporal"        "WorkingFamilies" "Freedom"        
# [13] "Equality"        "Misc"            "Legislation"       
 

par(mfrow=c(5, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")

topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])

speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeBush", type=="nomin",Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1], 
             xlab="Sentences", ylab="Topic share", main="George Bush, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="WilliamJClinton", type=="nomin", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Bill Clinton, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeWBush", type=="nomin", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1], 
            xlab="Sentences", ylab="Topic share", main="George W Bush, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="BarackObama", type=="nomin", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Barack Obama, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="DonaldJTrump", type=="nomin")%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Donald Trump, Nomination")
```

```{r, fig.width=3.3, fig.height=5}
# [1] "Economy"         "America"         "Defense"         "Belief"         
# [5] "Election"        "Patriotism"      "Unity"           "Government"     
# [9] "Reform"          "Temporal"        "WorkingFamilies" "Freedom"        
# [13] "Equality"        "Misc"            "Legislation"       


par(mfrow=c(5, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")


topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])


```{r}
speech.df=tbl_df(corpus.list.df)%>%filter(type=="nomin", word.count<20)%>%select(sentences, Economy:Legislation)

as.character(speech.df$sentences[apply(as.data.frame(speech.df[,-1]), 2, which.max)])

names(speech.df)[-1]

```


```{r, fig.width=3, fig.height=3}
presid.summary=tbl_df(corpus.list.df)%>%
  filter(type=="inaug", File%in%sel.comparison)%>%
  select(File, Economy:Legislation)%>%
  group_by(File)%>%
  summarise_each(funs(mean))

presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
              5)
fviz_cluster(km.res, 
             stand=T, repel= TRUE,
             data = presid.summary[,-1],
             show.clust.cent=FALSE)
```






















```{r,eval=FALSE,include=FALSE}
#Topic Modeling
#REMOVE WHITE SPACE FROM CORPUS
ff.all<-tm_map(ff.all, stripWhitespace)

#TRANSFORM LETTERS TO LOWER CASE TO WAS OUR ANALYSIS
ff.all<-tm_map(ff.all, content_transformer(tolower))

#REMOVE COMMON STOPWORDS IN ENGLISH
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))

#REMOVE EMPTY CHARATERS
ff.all<-tm_map(ff.all, removeWords, character(0))

#REMOVE PUNCTIONS
ff.all<-tm_map(ff.all, removePunctuation)

#REMOVE NUMBERS
ff.all = tm_map(ff.all,removeNumbers)

#TURN INTO A DOCUMENT MATRIX
dtm <- DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm=tidy(dtm)

#REFERENCE:
#LINCOLN-1 1 LINCOLN-2 2 
#WoodrowWilson-1 56 WoodrowWilson-2 57
#FranklinDroosevelt-2 13 FranklinDroosevelt-3 14 FranklinDroosevelt-4 15
#JohnFKenndy-1 35
#RonaldReagan-1 41
speech_selected=c("AbrahamLincoln-1","AbrahamLincoln-2","WoodrowWilson-1","WoodrowWilson-2","FranklinDroosevelt-2","FranklinDroosevelt-3","FranklinDroosevelt-4","JohnFKenndy-1","RonaldReagan-1")
#REMOVE SPASRE WORD
#dtms = removeSparseTerms(dtm,0.1)

#WRITE IT INTO A CSV
#write.csv(as.matrix(dtm),file="dtm.csv")
#dates = read.table("InauguationDatesEdit.txt",header = TRUE)
#Add Year Column to classify years this speech is given
#tdm.tidy=tidy(tdm.all)
#tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
#source("D:/Columbia University/Spring2017-Applied Data Science/Project_1_Bz2290/Spr2017-Proj1-bz2290/data/negative-words.txt")
#source("D:/Columbia University/Spring2017-Applied Data Science/Project_1_Bz2290/Spr2017-Proj1-bz2290/data/negative-words.txt")
```

```{r}
#SPLIT SPEECH INTO SENTNECES

```





```{r}
#WORD CLOUD#
library(shiny)

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1',
                              speech_selected,
                              selected=speech_selected[1])),
        column(4, selectInput('speech2', 'Speech 2', speech_selected,
                              selected=speech_selected[1])),
        column(4, sliderInput('nwords', 'Number of words', 3,
                               min = 20, max = 200, value=30, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),

    server = function(input, output, session) {

      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[gsub("inaug|.txt","",ff.dtm$document)==as.character(input$speech1)],
             dtm.count1=ff.dtm$count[gsub("inaug|.txt","",ff.dtm$document)==as.character(input$speech1)],
             dtm.term2=ff.dtm$term[gsub("inaug|.txt","",ff.dtm$document)==as.character(input$speech2)],
             dtm.count2=ff.dtm$count[gsub("inaug|.txt","",ff.dtm$document)==as.character(input$speech2)])
      })

     output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech2)
      })
    },

    options = list(height = 300)
)
```

